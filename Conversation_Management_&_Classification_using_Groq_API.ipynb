{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-5u3znM_CDu",
        "outputId": "49377e05-7b61-452e-d5bb-60e19c65a883"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.108.0)\n",
            "Collecting groq\n",
            "  Downloading groq-0.31.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.9)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
            "Downloading groq-0.31.1-py3-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.31.1\n",
            "Conversation History after 3 turns + summarization:\n",
            "[{'role': 'system', 'content': \"Summary so far: A user initiated a conversation with an assistant to inquire about their booking. The assistant responded by requesting the user's booking ID to proceed. The user provided the booking ID, which is 12345. The conversation is ongoing, with the assistant likely to provide information about the booking next.\"}, {'role': 'user', 'content': \"Yes, it's 12345.\"}]\n",
            "Extracted Info: {\"age\":29,\"email\":\"alice@gmail.com\",\"location\":\"New York\",\"name\":\"Alice Johnson\",\"phone\":\"9876543210\"}\n"
          ]
        }
      ],
      "source": [
        "!pip install openai groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "from groq import Groq\n",
        "\n",
        "# ✅ Setup Groq client (OpenAI-compatible)\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
        "\n",
        "# --------------------------\n",
        "# Task 1: Conversation Management\n",
        "# --------------------------\n",
        "conversation_history = []\n",
        "summary_counter = 0\n",
        "K = 3  # summarize after every 3 turns\n",
        "\n",
        "def add_message(role, content):\n",
        "    global summary_counter, conversation_history\n",
        "    conversation_history.append({\"role\": role, \"content\": content})\n",
        "    summary_counter += 1\n",
        "\n",
        "    # Perform summarization every K turns\n",
        "    if summary_counter % K == 0:\n",
        "        summarize_conversation()\n",
        "\n",
        "def summarize_conversation():\n",
        "    global conversation_history\n",
        "    text = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in conversation_history])\n",
        "    prompt = f\"Summarize the following conversation in 3-4 sentences:\\n{text}\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    summary = response.choices[0].message.content\n",
        "    # Replace history with summary + keep last 1 message\n",
        "    conversation_history = [{\"role\": \"system\", \"content\": f\"Summary so far: {summary}\"},\n",
        "                            conversation_history[-1]]\n",
        "\n",
        "def truncate_history_by_turns(n=4):\n",
        "    global conversation_history\n",
        "    conversation_history = conversation_history[-n:]\n",
        "\n",
        "def truncate_history_by_chars(limit=200):\n",
        "    global conversation_history\n",
        "    text = \"\\n\".join([m['content'] for m in conversation_history])\n",
        "    if len(text) > limit:\n",
        "        conversation_history = [{\"role\": \"system\", \"content\": text[-limit:]}]\n",
        "\n",
        "# --------------------------\n",
        "# Task 2: JSON Schema Extraction\n",
        "# --------------------------\n",
        "functions = [\n",
        "    {\n",
        "        \"name\": \"extract_user_info\",\n",
        "        \"description\": \"Extract user details from chat\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"name\": {\"type\": \"string\"},\n",
        "                \"email\": {\"type\": \"string\"},\n",
        "                \"phone\": {\"type\": \"string\"},\n",
        "                \"location\": {\"type\": \"string\"},\n",
        "                \"age\": {\"type\": \"integer\"}\n",
        "            },\n",
        "            \"required\": [\"name\", \"email\", \"phone\", \"location\", \"age\"]\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "def extract_info(chat_text):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "        messages=[{\"role\": \"user\", \"content\": chat_text}],\n",
        "        functions=functions,\n",
        "        function_call={\"name\": \"extract_user_info\"}\n",
        "    )\n",
        "    return response.choices[0].message.function_call.arguments\n",
        "\n",
        "# --------------------------\n",
        "# DEMONSTRATION\n",
        "# --------------------------\n",
        "# Task 1 Demo\n",
        "add_message(\"user\", \"Hi, I want to know about my booking.\")\n",
        "add_message(\"assistant\", \"Sure, can you give me your booking ID?\")\n",
        "add_message(\"user\", \"Yes, it's 12345.\")  # → summarization happens here\n",
        "\n",
        "print(\"Conversation History after 3 turns + summarization:\")\n",
        "print(conversation_history)\n",
        "\n",
        "# Task 2 Demo\n",
        "sample_chat = \"My name is Alice Johnson, email alice@gmail.com, phone 9876543210, I live in New York, and I’m 29 years old.\"\n",
        "print(\"Extracted Info:\", extract_info(sample_chat))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udNJbY-B_w_i",
        "outputId": "0c825e92-0d62-43c0-9972-f6a5e5b1c94a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversation History after 3 turns + summarization:\n",
            "[{'role': 'system', 'content': \"Summary so far: A user initiated a conversation to inquire about their booking. The assistant responded by requesting the user's booking ID to access the relevant information. The user provided the booking ID, which is 12345, to help the assistant locate their booking details. The conversation is ongoing, with the assistant likely to provide an update on the booking next.\"}, {'role': 'user', 'content': \"Yes, it's 12345.\"}]\n",
            "Extracted Info: {\"age\":29,\"email\":\"alice@gmail.com\",\"location\":\"New York\",\"name\":\"Alice Johnson\",\"phone\":\"9876543210\"}\n"
          ]
        }
      ]
    }
  ]
}